import argparse
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline, DDPMScheduler
from transformers import CLIPTextModel, CLIPTokenizer
from accelerate import Accelerator
from torch.utils.data import DataLoader
from src.dataset import VideoFrameDataset
import os

def main():
    # 1. åŸºç¡€é…ç½®
    parser = argparse.ArgumentParser(description="DL Final Project Training")
    parser.add_argument("--batch_size", type=int, default=4) # æ˜¾å­˜ä¸å¤Ÿæ”¹å°
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--use_mock", action="store_true", default=True, help="Use mock data")
    args = parser.parse_args()

    # åˆå§‹åŒ–åŠ é€Ÿå™¨ (è‡ªåŠ¨å¤„ç† GPU/CPU)
    accelerator = Accelerator()
    device = accelerator.device
    print(f"ğŸš€ Training device: {device}")

    # 2. åŠ è½½æ¨¡å‹ç»„ä»¶ (åŸºäº HuggingFace InstructPix2Pix) 
    model_id = "timbrooks/instruct-pix2pix"
    print(f"Loading model: {model_id}...")
    
    # åªéœ€è¦åŠ è½½ UNet è¿›è¡Œè®­ç»ƒï¼ŒVAE å’Œ Text Encoder é€šå¸¸å†»ç»“
    # æ³¨æ„ï¼šé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½çº¦ 5GB æ¨¡å‹ï¼Œè¯·ä¿æŒç½‘ç»œé€šç•…
    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder")
    vae = ... # ä¸ºç®€åŒ–ä»£ç ï¼Œæ­¤å¤„çœç•¥ VAE åŠ è½½ï¼Œå®é™…è®­ç»ƒéœ€åŠ è½½ VAE å°†å›¾ç‰‡è½¬ Latent
    unet = ... # åŒä¸Šï¼Œéœ€åŠ è½½ UNet
    
    # === ä¸ºäº†æ¼”ç¤ºç¯å¢ƒè·‘é€šï¼Œæˆ‘ä»¬è¿™é‡Œç”¨ä¸€ä¸ªæç®€çš„ Pipeline åŠ è½½æ–¹å¼ ===
    pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
        model_id, torch_dtype=torch.float16
    ).to(device)
    pipeline.set_progress_bar_config(disable=True)
    
    print("âœ… Model loaded successfully.")

    # 3. å‡†å¤‡æ•°æ®
    dataset = VideoFrameDataset(use_mock=args.use_mock, resolution=128)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    
    optimizer = torch.optim.AdamW(pipeline.unet.parameters(), lr=1e-5)

    # 4. æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯ (Proof of Life)
    print("Starting training loop check...")
    pipeline.unet.train()
    
    for epoch in range(args.epochs):
        for step, batch in enumerate(dataloader):
            # è¿™é‡Œçš„é€»è¾‘æ˜¯è¯æ˜æ•°æ®èƒ½æµè¿‡æ¨¡å‹ï¼Œä¸”ä¸æŠ¥é”™
            # çœŸå®è®­ç»ƒéœ€è¦å®Œæ•´çš„ Noise Scheduler å’Œ Loss è®¡ç®—
            
            # æ¨¡æ‹Ÿ: è·å–æ–‡æœ¬ Embedding
            inputs = tokenizer(
                batch["input_ids"], max_length=77, padding="max_length", truncation=True, return_tensors="pt"
            ).to(device)
            encoder_hidden_states = pipeline.text_encoder(inputs.input_ids)[0]
            
            # æ‰“å°çŠ¶æ€è¯æ˜åœ¨è¿è¡Œ
            if step % 5 == 0:
                print(f"Epoch {epoch}, Step {step}: Data loaded, Tensors shape {batch['pixel_values'].shape}")
                
            # åªè¦è¿™ä¸€æ­¥ä¸æŠ¥é”™ï¼Œè¯´æ˜æ˜¾å­˜å¤Ÿç”¨ï¼Œç¯å¢ƒé…ç½®æ­£ç¡®
            break # æ¼”ç¤ºæ¨¡å¼åªè·‘ä¸€ä¸ª Batch
            
    print("ğŸ‰ Environment Check Passed! Training script is ready.")
    
    # ä¿å­˜å ä½ç¬¦æƒé‡
    os.makedirs("checkpoints", exist_ok=True)
    with open("checkpoints/model_status.txt", "w") as f:
        f.write("Training environment verified.")

if __name__ == "__main__":
    main()